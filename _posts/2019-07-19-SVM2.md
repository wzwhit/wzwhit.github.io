---
layout:     post
title:      "非线性SVM与核函数"
subtitle:   ""
date:       2019-07-19 11:00:00
author:     "wzw"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:     true
tags:
    - DeepLearning
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

svm学习笔记，参考《统计学习方法》

## 线性支持向量机

[传送门][线性svm]

## 非线性支持向量机与核函数

如果能用一个**超曲面**将数据集中正负例分开，则称这个问题为非线性可分问题。

![31.png]({{ site.url }}/img/svm/svm-31.png)

对于上图中的例子，我们可以用一个椭圆形的超曲面将数据集分开，是一个非线性可分问题。

非线性问题往往不好解，如果能通过一个非线性变换，将左图中椭圆变成右图中的直线，就可将非线性分类问题变换为线性分类问题。

![32.png]({{ site.url }}/img/svm/svm-32.png)

所以对于非线性分类问题，我们可以用线性分类方法来求解：首先使用一个变换将原空间的数据映射到新空间，然后在新空间里用线性分类学习方法从训练数据中学习分类模型。因此我们引入核函数到svm中，来使用svm解决非线性分类问题。

#### 核函数

![33.png]({{ site.url }}/img/svm/svm-33.png)

![34.png]({{ site.url }}/img/svm/svm-34.png)

通常我们直接计算 K(x,z) 比较简单，所以我们在学习和预测中只定义核函数，而不显示的定义映射函数
$$
\phi
$$
。

在线性svm中，在对偶问题的目标函数(7.37)(见上一篇[传送门][线性svm])中的内积
$$
x_i\cdot x_j
$$
可以用核函数
$$
K(x_i,x_j)=\phi(x_i)\cdot\phi(x_j)
$$
来代替，此时对偶问题的目标函数变为：

![35.png]({{ site.url }}/img/svm/svm-35.png)

而分类决策函数式变为：

![36.png]({{ site.url }}/img/svm/svm-36.png)

这等价于经过映射函数
$$
\phi
$$
将输入空间中的内积
$$
x_i,x_j
$$
变换为特征空间中的内积
$$
\phi(x_i)\cdot\phi(x_j)
$$
，在新的特征空间中从训练样本中学习线性svm。当映射函数是非线性函数时，学习到的含有核函数的svm就是非线性分类模型。

学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数，这样的技巧称为核技巧。

##### 常用的核函数

![37.png]({{ site.url }}/img/svm/svm-37.png)

……等等等

对于设计的函数 K(x,z) 是否是否可以作为核函数，可以通过正定核的充要条件(太长了不看了)来判断。而在实际应用中，往往依赖领域知识直接选择核函数，然后通过实验验证选择的核函数是否有效。

## 序列最小最优化(SMO)算法

svm的学习问题可以转化为求解凸二次规划问题，但是当样本数很大时，svm算法往往变得非常低效。如何高效地实现svm的学习呢？一种快速实现算法就是序列最小最优化(SMO)算法(有空再看)。

[线性svm]: https://wzwhit.github.io/2019/07/18/SVM1/

